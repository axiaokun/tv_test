# 岭回归
岭回归是线性回归的正则化版，相当于在成本函数中添加一个约束项。
$$\lambda\sum_{i=1}^n\theta_i^2$$
注意的是这一项不是从i=0开始，因为一般默认是不对偏置项进行约束的。

有了这一个正则项，可以使得模型不仅需要拟合数据，同时还要使得模型的权重最小（这里查了一下原因说是：当自变量存在复共线性时，回归系数估计的方差就很大，估计值就很不稳定。那么系数也就是这里的权重，那么就是为了使得回归系数估计的方差最小）

对于多出来的这一项中的超参数$\lambda$是用来控制对模型进行正则化的程度，当$\lambda = 0$时就等于没有了这一项，变成了原来的线性模型。如果$\lambda$很大时，那么所有的权重都会被处罚到接近零。

* **岭回归成本函数**  
  $$J(\theta) = MSE(\theta) + \frac{1}{2}\lambda\sum_{i=1}^n\theta_i^2$$

* 求偏导
  
  由于正则化过程中会减小除了$\theta_0$以外的参数，因此使用梯度下降法使这个代价函数最小化时，我们会分成两种情况：
 $$\theta_0:=\theta_0-a[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^i)-y^i)x_0^i]$$

  减数部分就是代价函数对$\theta_0$偏导的结果。

  $$\theta_j:=\theta_j-a[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^i)-y^i)x_j^i+\frac{\lambda}{m}\theta_j]$$

  这里的$j$是除去当$j=0$的情况。

在多变量线性回归中，可以先向量化这些变量，然后将式子改写成矩阵方程。同理这里只是添加了一个正则项，我们也可以将它正则化，然后依旧构建矩阵方程。

* 对于**多变量线性回归的正规方程**：

  $$\theta = (X^TX)^{-1}X^Ty$$

  首先是将代价方程向量表达式转变为矩阵表达式：

  $$j(\theta) = \frac{1}{2}(X\theta - y)^T(X\theta - y)$$

  将其展开后就得到下面这个式子：

  $$j(\theta) = \frac{1}{2}(\theta^TX^TX\theta-\theta^TX^Ty - y^TX\theta - y^Ty)$$

  接下来我们要用到几个矩阵求导的法则：

  $$\frac{dAB}{dB} = A^T$$
  $$\frac{dX^TAX}{dX} = 2AX$$
  $$\frac{dX^TB}{dX} = B$$

  接下来就只要将这些运用到上面得到的矩阵表达式的求导中就可以得到最后的式子：

  $$\frac{\partial j(\theta)}{\partial \theta} = X^TX\theta - X^Ty$$

  那么当代价函数最小化时，函数的偏导数会趋近于零。于是我们令上面这个式子的左端等于零，经过一些简单的移项就可以得出最后的答案：

  $$\theta = (X^TX)^{-1}X^Ty$$

  然后差不多还是按照同样的思路运用到岭回归模型中。首先，在岭回归的式子中相比之前的式子多出了一个惩罚项，那么在将式子转变为矩阵表达式时也要把这一项处理一下，我们可以将$\lambda\sum_{j=1}^{n}\theta_j^2$转变为$\lambda \theta^T \theta$但此时这个$\theta$是不包括$\theta_0$的，所以我们还可以将其变成：

  $$\lambda(\begin{bmatrix}
      0 &\\  &  1 &  &  \\ & & 1\\ & &  & .\\ & & & & .\\ & & &　& & .\\ & & & & &  &1　\end{bmatrix}\theta)^T(\begin{bmatrix}0 \\  &  1 &  &  \\ & & 1\\ & &  & .\\ & & & & .\\ & & &　& & .\\ & & & & &  &1 \end{bmatrix}\theta)$$

  此时式子中的这个$\theta$就是由所有的参数组成的一个向量。为了方便书写这里令矩阵$B = \begin{bmatrix}0 \\  &  1 &  &  \\ & & 1\\ & &  & .\\ & & & & .\\ & & &　& & .\\ & & & & &  &1　\end{bmatrix}$

  所以此时的矩阵表达式为：

  $$j(\theta) = \frac{1}{2}[(X\theta - y)^T(X\theta - y) + \lambda \theta^T B^TB\theta]$$

  求导后就得到下面这个式子：

  $$\theta = (X^TX)^{-1}X^Ty + \lambda B^TB\theta$$

  注意这里$B^TB = B$接下来还是令偏导导数趋于零就可以得出最后的答案

  $$\theta = (X^TX+\lambda\begin{bmatrix}
      0 \\  &  1 &  &  \\ & & 1\\ & & & .\\ & & & & .\\ & & &　& & .\\ & & & & &  &1\end{bmatrix})^{-1}X^Ty$$